import re
import sys
import json

try:
    import requests
    from lxml import etree
except ModuleNotFoundError:
    print(
        "You need to install the following modules 'requests', 'lxml'.", file=sys.stderr
    )
    sys.exit(1)

try:
    from py_common import log
    from py_common.config import get_config
    from py_common.util import scraper_args
    from py_common.types import (
        ScrapedPerformer,
        ScrapedScene,
        ScrapedStudio,
        ScrapedTag,
    )
except ModuleNotFoundError:
    print(
        "You need to download the folder 'py_common' from the community repo! (CommunityScrapers/tree/master/scrapers/py_common)",
        file=sys.stderr,
    )
    sys.exit(1)

# DO NOT EDIT THIS FILE
# run the scraper once and edit the config.ini file instead
config = get_config(
    default="""
# Use the network console in your browsers developer tools to find the fc2ppvdb_session cookie after logging in.
fc2ppvdb_session = 
"""
)

BASE_QUERY_URL = "https://fc2ppvdb.com"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/26.0.1 Safari/605.1.15"
URL_SEARCH_PATTERN = r".+/articles/(\d{5,}).*"
CODE_SEARCH_PATTERN = r".*?(\d{5,}).*"


def extract_id(fragment: dict) -> str:
    if url := fragment.get("url"):
        search = re.search(pattern=URL_SEARCH_PATTERN, string=url)
        if search:
            return search[1]

    if code := fragment.get("code"):
        search = re.search(pattern=CODE_SEARCH_PATTERN, string=code)
        if search:
            return search[1]

    if title := fragment.get("title"):
        search = re.search(pattern=CODE_SEARCH_PATTERN, string=title)
        if search:
            return search[1]

    for file in fragment.get("files", []):
        search = re.search(pattern=CODE_SEARCH_PATTERN, string=file["path"])
        if search:
            return search[1]

    return None


def export_scene(result: dict) -> ScrapedScene:
    article = result["article"]

    tags = []
    for article_tag in article["tags"]:
        tag = ScrapedTag(name=article_tag["name"])
        tags.append(tag)

    performers = []
    for article_performer in article["actresses"]:
        performer = ScrapedPerformer(
            name=article_performer["name"],
            urls=[f"{BASE_QUERY_URL}/actresses/{article_performer["id"]}"],
        )
        performers.append(performer)

    image_url = article["image_url"]
    if not image_url.startswith("https"):
        image_url = f"{BASE_QUERY_URL}{image_url}"

    scene = ScrapedScene(
        title=article["title"],
        date=article["release_date"],
        tags=tags,
        performers=performers,
        studio=ScrapedStudio(
            name=article["writer"]["name"],
            url=f"{BASE_QUERY_URL}/writers/{article["writer"]["slug"]}",
        ),
        director=article["writer"]["name"],
        code=f"FC2-PPV-{article["video_id"]}",
        image=image_url,
        url=f"{BASE_QUERY_URL}/articles/{article["video_id"]}",
    )

    return scene


def request_article(session: requests.Session, video_id: str) -> requests.Response:
    article_url = f"{BASE_QUERY_URL}/articles/{video_id}"

    try:
        response = session.get(article_url, timeout=10, verify=False)
    except requests.RequestException as req_error:
        log.error(f"Requests article failed: {req_error}")
        return None

    return response


def request_article_info(
    session: requests.Session, video_id: str, csrf: str
) -> requests.Response:
    try:
        xsrf = session.cookies.get_dict().get("XSRF-TOKEN")
    except:
        log.error("XSRF not found")
        return None

    session.headers.update(
        {
            "X-CSRF-TOKEN": csrf,
            "X-Requested-With": "XMLHttpRequest",
            "X-XSRF-TOKEN": xsrf,
        }
    )

    api_url = f"{BASE_QUERY_URL}/articles/article-info?videoid={video_id}"

    try:
        response = session.get(api_url, timeout=10, verify=False)
    except requests.RequestException as req_error:
        log.error(f"Request API failed: {req_error}")
        return None

    return response


def get_csrf(article_response: requests.Response) -> str:
    article_tree = etree.HTML(text=article_response.text)
    try:
        csrf = article_tree.xpath("//meta[@name='csrf-token']/@content")[0]
    except IndexError:
        log.error("CSRF not found")
        return None

    return csrf


def send_request(video_id: str) -> ScrapedScene:
    if not config.fc2ppvdb_session:
        log.error("Please configure fc2ppvdb_session in fc2ppvdb/config.ini")
        sys.exit(1)

    log.debug(f"Using cookie: {config.fc2ppvdb_session}")

    requests.packages.urllib3.disable_warnings()
    session = requests.Session()

    session.cookies.update(
        {
            "fc2ppvdb_session": config.fc2ppvdb_session,
        }
    )

    session.headers.update({"User-Agent": USER_AGENT})

    page_response = request_article(session, video_id)
    if page_response is None:
        sys.exit(1)

    csrf = get_csrf(page_response)
    if csrf is None:
        sys.exit(1)

    article_response = request_article_info(session, video_id, csrf)
    if article_response is None:
        sys.exit(1)

    try:
        result = article_response.json()
    except json.decoder.JSONDecodeError as json_error:
        log.error(f"Failed to decode article info, reason: {json_error}")
        log.debug(f"Response: {article_response.text}")
        sys.exit(1)

    log.debug(f"Receive object: {result}")

    return export_scene(result)


if __name__ == "__main__":
    op, args = scraper_args()

    match op, args:
        case "scene-by-fragment" | "scene-by-url", args:
            video_id = extract_id(args)
        case _:
            log.error(f"Operation: {op}, arguments: {json.dumps(args)}")
            sys.exit(1)

    if not video_id:
        log.error("Failed to extract id from input")
        sys.exit(1)

    result = send_request(video_id)
    output = json.dumps(result)
    print(output)
